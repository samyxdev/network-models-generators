{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a052eec8",
   "metadata": {},
   "source": [
    "# Barabasi-Albert model (BA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32271d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random as rd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats # to do regression for the estimation of the exponent\n",
    "from scipy.stats import poisson\n",
    "from network_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97870017",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "The algorithm has 3 input parameters: \n",
    "* n_0: number of initial nodes\n",
    "* N: number of final nodes\n",
    "* m: number of links for each new added node (should be m < m_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d692ad",
   "metadata": {},
   "source": [
    "Insert the parameters of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979aa466",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert he number of initial nodes:\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# Insert Parameters\n",
    "n_0 = int(input(\"Insert he number of initial nodes:\\n\"))\n",
    "N = int(input(\"\\nInsert number of final nodes:\\n\"))\n",
    "m = int(input(\"\\nInsert in the value of m parameter:\\n\"))\n",
    "if m < 1 or  m >=N:\n",
    "        raise nx.NetworkXError(\"Barabási–Albert network must have m >= 1\"\n",
    "                               \" and m < n, m = %d, n = %d\" % (m, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b0b497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# name of the model file\n",
    "model_name = \"nets/BA_n020_m5_n10000.net\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34738592",
   "metadata": {},
   "source": [
    "Let's now create the initial graph: we build a complete graph with m_0 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faec4483",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.complete_graph(n_0) # nodes in the range from 0 to m_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dc75ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(G.nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd1bf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list the nodes\n",
    "G.nodes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5409d37",
   "metadata": {},
   "source": [
    "We now need to add incrementally nodes until N (we'll add nodes by increasing the number/label identifying each node). \n",
    "We use a loop to do that. After adding a node we need to add m links from it to the existing nodes, we'll use another (nested) loop to do that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fea760",
   "metadata": {},
   "source": [
    "## Auxiliary functions\n",
    "Before implementing the actual code of the algorithm, lets define some helper functions for creating m links between the newly added node and the existing nodes according to the prefential attachment mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c122f3",
   "metadata": {},
   "source": [
    "The first function we'll define builds the weighted probabilities of the graph nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4492166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the list of the weighted probability of each existing node\n",
    "def build_probs(G):\n",
    "    nodes_probs = [] # our list\n",
    "                            \n",
    "    # compute the probability for each node\n",
    "    for node in G.nodes():\n",
    "        node_deg = G.degree(node)\n",
    "        \n",
    "        # probability of the node\n",
    "        node_prob = node_deg / (2 * len(G.edges)) # N.B.: 2 * len(G.edges) returns the sum of the degrees of all the nodes, since each edge contributes to the degree of 2 nodes\n",
    "        \n",
    "        # insert into the list\n",
    "        nodes_probs.append(node_prob)\n",
    "    \n",
    "    return nodes_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493f86fa",
   "metadata": {},
   "source": [
    "The 2nd auxiliary function adds m links between a node and the existing nodes, according to the preferential attachment.\n",
    "We use the random.choice() function of the numpy module, which takes as input the list of nodes among which to select one and the probability associated to each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb8247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_links(G, added_node):\n",
    "    selected_nodes = [] # list of the selected nodes\n",
    "    \n",
    "    # before inserting the links we build the weighted probabilities of each node\n",
    "    nodes_probs = build_probs(G)\n",
    "    \n",
    "    # add m links\n",
    "    for added_links in range(m):\n",
    "        # we use the choice function of the numpy.random module\n",
    "        selected_node = np.random.choice(G.nodes(),p=nodes_probs) # selects a node according to preferential attachment\n",
    "        \n",
    "        # check whether the node was already selected (to avoid multiple links)\n",
    "        while selected_node in selected_nodes:\n",
    "            selected_node = np.random.choice(G.nodes(),p=nodes_probs) # select another node\n",
    "        \n",
    "        # once we selected the node we should create the link among the added_node given as a parameter\n",
    "        # and the selected_node\n",
    "        G.add_edge(added_node, selected_node)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965bdc2a",
   "metadata": {},
   "source": [
    "## Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41450e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_to_add = N - n_0 # number of nodes to add until reaching N total nodes\n",
    "inserted_nodes = 0 # counter of the inserted nodes\n",
    "\n",
    "for n_i in range(nodes_to_add):\n",
    "     \n",
    "    # insert new_node in the graph\n",
    "    print(\"--------------- STEP: {} ---------------\".format(inserted_nodes + 1))\n",
    "    G.add_node(n_0 + inserted_nodes) # we add the node by specifying its numeric value (starting from 50)\n",
    "    added_node = n_0 + inserted_nodes # identifier of the added node\n",
    "    inserted_nodes += 1\n",
    "    \n",
    "    print(\"Node added: {}\".format(added_node))\n",
    "    # add the newly inserted node into nodes_probs because of the random.choice function to work\n",
    "    \n",
    "    # --------------------------------------------------------------\n",
    "\n",
    "    # after adding the node, we need to create m links with the existing nodes \n",
    "    # according to the preferential attachment mechanism\n",
    "    \n",
    "    # function to add m links to the newly inserted node\n",
    "    add_links(G, added_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68ce48",
   "metadata": {},
   "source": [
    "### Save the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545841e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_pajek(G, model_name)\n",
    "#nx.write_gexf(G, \"test.gexf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7375a6c8",
   "metadata": {},
   "source": [
    "## Network descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfa5479",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of nodes: {}\".format(G.number_of_nodes()))\n",
    "#G.nodes()\n",
    "\n",
    "print(\"Number of edges: {}\".format(G.number_of_edges()))\n",
    "#G.edges()\n",
    "\n",
    "degrees_values = [val for key,val in G.degree()] # degree for each node\n",
    "#degrees_values\n",
    "\n",
    "print(\"Min degree: {}\".format(np.min(degrees_values)))\n",
    "print(\"Max degree: {}\".format(np.max(degrees_values)))\n",
    "print(\"Avg degree: {}\".format(round(np.mean(degrees_values), 4)))\n",
    "G = nx.Graph(G)\n",
    "\n",
    "print(\"Clustering: {}\".format(round(nx.average_clustering(G), 4)))\n",
    "print(\"Assortativity: {}\".format(round(nx.degree_assortativity_coefficient(G), 4)))\n",
    "print(\"Avg path length: {}\".format(round(nx.average_shortest_path_length(G), 4)))\n",
    "print(\"Diameter: {}\".format(nx.diameter(G)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f2f011",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16ac001",
   "metadata": {},
   "source": [
    "### Plotting the network\n",
    "Plot only small size networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81248d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if N <= 100:\n",
    "    nx.draw(G, alpha = .3, edge_color = '#40a6d1', node_color = '#40a6d1', node_size=30, with_labels = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a6d84c",
   "metadata": {},
   "source": [
    "### Plotting the distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eaa07e",
   "metadata": {},
   "source": [
    "We use the auxiliary functions we defined in network_utils for visualizing the distribution using either linear or log-log scale. <br>\n",
    "\n",
    "We know that the degree distribution of Barabási–Albert network is k<sup>-3</sup> and so it gives a straight line in log-log scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37adf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_distrib_lin(graph=G, colour='#40a6d1', alpha=.8)\n",
    "plt.subplot(1, 2, 2)\n",
    "plot_distrib_log(graph=G, colour='#40a6d1', alpha=.8, fit_line=True, expct_lo=5, expct_hi=100, expct_const=60)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a7645",
   "metadata": {},
   "source": [
    "### Degree distribution histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c70d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary function to create logaritmically spaced bins (for log-log histogram) by specifying the number of bins\n",
    "def create_log_bins(degrees, num = 20):\n",
    "    bins = np.logspace(np.log10(np.min(degrees)), np.log10(np.max(degrees)), num)\n",
    "    bins = np.array(bins)\n",
    "    return bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b781609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF histogram in linear scale\n",
    "def plot_linear_PDF(G, name='', nbins = 15):\n",
    "    degrees = [G.degree(n) for n in G.nodes()]\n",
    "    #plt.figure(figsize=(12,8))\n",
    "    plt.title('PDF in linear scale', fontsize=15)\n",
    "    plt.xlabel('Degree', fontsize=13)\n",
    "    plt.ylabel('PDF', fontsize=13)\n",
    "    plt.hist(degrees, bins=nbins, density = True, cumulative = False)\n",
    "    plt.tight_layout()\n",
    "    plt.style.use('ggplot')\n",
    "        \n",
    "#plot_linear_PDF(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42ea896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF histogram in Log-Log scale\n",
    "def plot_loglog_PDF(G, name=\"\", nbins=20):\n",
    "    degrees = [G.degree(n) for n in G.nodes()]\n",
    "    \n",
    "    # creating logaritmically spaced bins\n",
    "    bins = create_log_bins(degrees, num = nbins)\n",
    "    \n",
    "    #plt.figure(figsize=(12,8))\n",
    "    plt.title('PDF in log-log scale',  fontsize=15)\n",
    "    plt.xlabel('Degree', fontsize=13)\n",
    "    plt.ylabel('PDF', fontsize=13)\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.hist(degrees, bins=bins, density = True, cumulative = False)\n",
    "    plt.tight_layout()\n",
    "    #plt.style.use('ggplot')\n",
    "\n",
    "#plot_loglog_PDF(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333e29f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CCDF histogram in linear scale\n",
    "def plot_linear_CCDF(G, name=\"\", nbins=30):\n",
    "    degrees = [G.degree(n) for n in G.nodes()]\n",
    "    #plt.figure(figsize=(12,8))\n",
    "    plt.title('CCDF in linear scale', fontsize=15)\n",
    "    plt.xlabel('Degree', fontsize=13)\n",
    "    plt.ylabel('CCDF', fontsize=13)\n",
    "    plt.hist(degrees, bins=nbins, density = True, cumulative = -1)\n",
    "    plt.tight_layout()\n",
    "    #plt.style.use('ggplot')\n",
    "\n",
    "#plot_linear_CCDF(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cf0e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CCDF histogram in Log-Log scale\n",
    "def plot_loglog_CCDF(G, name=\"\", nbins=30):\n",
    "    degrees = [G.degree(n) for n in G.nodes()]\n",
    "    \n",
    "    # creating logaritmically spaced bins\n",
    "    bins = create_log_bins(degrees, num=nbins)\n",
    "    \n",
    "    #plt.figure(figsize=(12,8))\n",
    "    plt.title('CCDF in log-log scale', fontsize=15)\n",
    "    plt.xlabel('Degree', fontsize=13)\n",
    "    plt.ylabel('CCDF', fontsize=13)\n",
    "    plt.yscale('log')\n",
    "    plt.xscale('log')\n",
    "    plt.hist(degrees, bins=bins, density = True, cumulative = -1)\n",
    "    plt.tight_layout()\n",
    "    #plt.style.use('ggplot')\n",
    "    \n",
    "#plot_loglog_CCDF(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bc5043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of bins to use\n",
    "n_bins = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cdcb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_linear_PDF(G, nbins=n_bins) \n",
    "plt.subplot(1, 2, 2)\n",
    "plot_loglog_PDF(G, nbins=n_bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82529b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plot_linear_CCDF(G, nbins=n_bins) \n",
    "plt.subplot(1, 2, 2)\n",
    "plot_loglog_CCDF(G, nbins=n_bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30458cb",
   "metadata": {},
   "source": [
    "## Estimation of the exponent for the empirical degree distributions of BA and CM(SF), for the networks of size N>=1000.\n",
    "We'll see 3 main approaches for estimating the exponent of a scale-free network:\n",
    "* MLE\n",
    "* Linear regression on the binned PDF histogram\n",
    "* Linear regression on the binned CCDF histogram\n",
    "\n",
    "Let's see these three approaches and then we'll compare the results obtained by each one of them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332d6505",
   "metadata": {},
   "source": [
    "### Using MLE\n",
    "* We can use the <i>powerlaw</i> Python package described here: https://arxiv.org/abs/1305.0215, which is based on a MLE of the exponent of a powerlaw distribution . <br>\n",
    "As described in the paper https://arxiv.org/abs/0706.1062, the <i>powerlaw</i> package we use performs the exponent estimation by following the formula 3.7 <br>\n",
    "According to the paper, the first step to perform a maximum likelihood fit to a powerlaw is to determine what portion of the data to fit. A heavy-tailed\n",
    "distribution's interesting feature is, indeed, the tail and its properties, so if the initial, small values of the data\n",
    "do not follow a power law distribution we may opt to disregard them. The question is from what\n",
    "minimal value xmin (Kmin) the scaling relationship of the power law begins. The methods descibed in the paper of Clauset et al. mentioned above find this optimal value of xmin by creating a powerlaw fit starting from each unique value in the dataset, then selecting the one that results in the minimal Kolmogorov-Smirnov distance, D, between the data and the fit. Therefore the algorithm finds xmin my minimizing D. <br> <br>\n",
    "* While the maximum likelihood fit to a continous power law can be calculated analytically,\n",
    "and thus the optimal xmin and resulting fitted parameters can be computed quickly, this is not so for the discrete case. The maximum likelihood fit for a discrete power law is found by numerical optimization,\n",
    "the computation of which for every possible value of xmin can take time. To circumvent this issue,\n",
    "powerlaw can use an analytic estimate of gamma, from [https://arxiv.org/abs/0706.1062], which can \"give results accurate to about 1% or\n",
    "better provided xmin ≥ 6\".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132909cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install powerlaw\n",
    "#pip install mpmath "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f4942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import powerlaw\n",
    "\n",
    "degrees = [G.degree(n) for n in G.nodes()]\n",
    "results = powerlaw.Fit(degrees, discrete = True)\n",
    "print(\"\\n\\nExponent estimation:\")\n",
    "print(round(results.power_law.alpha, 4))\n",
    "#print(results.power_law.xmin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442c9cbf",
   "metadata": {},
   "source": [
    "### Using Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b38a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find min and max of the degrees\n",
    "Kmin = np.min(degrees)\n",
    "Kmax = np.max(degrees)\n",
    "\n",
    "Kmin, Kmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd2e4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the logarithm of K_i for all the data elements\n",
    "log_degrees = [np.log10(G.degree(n)) for n in G.nodes()]\n",
    "#log_degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3446a812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate useful logs\n",
    "log_Kmin = np.log10(Kmin)\n",
    "#log_Kmax = np.log10(Kmax)\n",
    "log_Kmax_1 = np.log10(Kmax + 1)\n",
    "log_Kmin, log_Kmax_1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0752e7",
   "metadata": {},
   "source": [
    "Divide the interval in equal size bins and build the bind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5421d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of the interval\n",
    "interval_length = (log_Kmax_1 - log_Kmin) / n_bins\n",
    "\n",
    "# build the bins\n",
    "bins = []\n",
    "for i in range(n_bins):\n",
    "    bins.append(log_Kmin + interval_length * i)\n",
    "bins.append(log_Kmax_1)\n",
    "bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1efe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count how many elements ki have their log(ki) in each bin\n",
    "counts = []\n",
    "for i in range(n_bins):\n",
    "    counts.append(len([deg for deg in log_degrees if deg >= bins[i] and deg < bins[i+1]]))\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6916cc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividing the number of elements in each bin by the total number of elements n we get estimations for the probabilities p_b of each bin\n",
    "probs = [deg/ N for deg in counts]\n",
    "probs # probabilities of each bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cf2bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddf8936",
   "metadata": {},
   "source": [
    "### Linear regression on the PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e5c5a1",
   "metadata": {},
   "source": [
    "We now need to make the linear regression of pairs (x_b, log_pb) to obtain the regression line. <br>\n",
    "Let's first calculate the elements of log_pb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecd653e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_PDF_p = [np.log10(p) for p in probs]\n",
    "log_PDF_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8018ec8e",
   "metadata": {},
   "source": [
    "We may have some infinite values: let's remove them from the list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17232c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list containing indices of the infinite elements\n",
    "inf_indices = []\n",
    "\n",
    "# populate the list\n",
    "for i in range(len(log_PDF_p)):\n",
    "    if math.isinf(log_PDF_p[i]):\n",
    "        inf_indices.append(i)\n",
    "               \n",
    "inf_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5c5f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only finite elements:\n",
    "log_PDF_p = [v for v in log_PDF_p if not math.isinf(v)]\n",
    "log_PDF_p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efafb8f4",
   "metadata": {},
   "source": [
    "The X vector of the linear regression is the bins vector, except from the first element <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ea6d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bins[1:]\n",
    "\n",
    "# we should select only elements corresponding to finite values of the correspondig log_PDF_p\n",
    "x_2 = []\n",
    "for i in range(len(X)):\n",
    "    if i not in inf_indices:\n",
    "        x_2.append(X[i])\n",
    "X = x_2\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2473d7",
   "metadata": {},
   "source": [
    "The Y vector of the linear regression is log_PDF_pLog_pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f03742",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = log_PDF_p\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ab5366",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04766d98",
   "metadata": {},
   "source": [
    "Let's perform the Linear Regression by using the stats function of the scipy package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfae19b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = stats.linregress(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e3fb1c",
   "metadata": {},
   "source": [
    "We are interested in the slope of the regression line, indeed the estimation of the exponent is: <br>\n",
    "* gamma = -slope + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49827025",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Slope: \", regr.slope)\n",
    "exponent_estimation =  - regr.slope + 1\n",
    "print(\"\\nExponent estimation:\", exponent_estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f5d40c",
   "metadata": {},
   "source": [
    "### Linear regression on the CCDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da42eb4",
   "metadata": {},
   "source": [
    "Another alternative for the estimation of exponent gamma consists in doing exactly the same as above but for the complementary cumulative distribution function (CCDF) instead of the probability density function (PDF). <br>\n",
    "CCDF is calculated from the PDF just by summing up \n",
    "the probabilities of all the bins to the right of the bin you are considering (this one included in the sum). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299416e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# probabilities of the bins\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f1152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute CCDF by summing on the right\n",
    "CCDF_c = []\n",
    "for i in range(len(probs)):\n",
    "    CCDF_c.append(np.sum(probs[i:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badfd093",
   "metadata": {},
   "outputs": [],
   "source": [
    "CCDF_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78df964",
   "metadata": {},
   "source": [
    "We now need to make the linear regression of pairs (x_b and log_pb) to obtain the regression line.<br>\n",
    "Let's first calculate the elements of log(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "826e71da",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_CCDF_c = [np.log10(c) for c in CCDF_c]\n",
    "log_CCDF_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8369a9",
   "metadata": {},
   "source": [
    "Again, as before, we may have infinite values that we should remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d962e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list containing indices of the infinite elements\n",
    "inf_indices = []\n",
    "\n",
    "for i in range(len(log_CCDF_c)):\n",
    "    if math.isinf(log_CCDF_c[i]):\n",
    "        inf_indices.append(i)\n",
    "               \n",
    "inf_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137e43a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only finite elements:\n",
    "log_CCDF_c = [v for v in log_CCDF_c if not math.isinf(v)]\n",
    "log_CCDF_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c669ecf",
   "metadata": {},
   "source": [
    "X vector is the bins vector, except from the first element <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0026f09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bins[1:]\n",
    "\n",
    "# we should select only elements corresponding to finite values of the correspondig log_PDF_p\n",
    "x_2 = []\n",
    "for i in range(len(X)):\n",
    "    if i not in inf_indices:\n",
    "        x_2.append(X[i])\n",
    "X = x_2\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4286259c",
   "metadata": {},
   "source": [
    "Y vector is the log_CCDF_c vector computed above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd1d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = log_CCDF_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381d3016",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c7f928",
   "metadata": {},
   "source": [
    "Let's perform the Linear Regression by using the stats function of the scipy package: the procedure is the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc522ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = stats.linregress(X, Y)\n",
    "print(\"Slope: \", regr.slope)\n",
    "exponent_estimation =  - regr.slope + 1\n",
    "print(\"\\nExponent estimation:\", exponent_estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b70b55",
   "metadata": {},
   "source": [
    "#### Conclusion on exponent estimation\n",
    "Binning-based approaches, i.e. the 2 approaches we saw based on constructing bins and performing regression on the binned data,  perform poorly. <br>\n",
    "As we can easily imagine, in binning-based approaches the estimated exponent is highly dependent on the choice of bin width, and this dependency varies as a function of sample size ( [https://esajournals.onlinelibrary.wiley.com/doi/10.1890/07-1288.1White]). <br>\n",
    "In general, binning results in a loss of\n",
    "information about the distributions of points within a\n",
    "bin and is thus expected to perform poorly (Clauset et al.\n",
    "2007, Edwards et al. 2007). Therefore, while binning is\n",
    "useful for visualizing the frequency distribution, and\n",
    "normalized logarithmic binning performs well at this\n",
    "task, binning-based approaches should be avoided for\n",
    "parameter estimation (Clauset et al. 2007). <br>\n",
    "\n",
    "Maximum likelihood estimation performs best in estimating the powerlaw exponent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
